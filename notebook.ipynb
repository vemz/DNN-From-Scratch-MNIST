{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96d972b4",
   "metadata": {},
   "source": [
    "Importation des librairies et Chargement des Données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b62c4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Configuration aléatoire pour la reproductibilité\n",
    "np.random.seed(42)\n",
    "\n",
    "def lire_alpha_digit(indices=None):\n",
    "    \"\"\"\n",
    "    Fonction pour lire la base Binary AlphaDigits.\n",
    "    Charge le fichier 'binaryalphadigs.mat' qui doit être présent dans le dossier.\n",
    "    Arguments:\n",
    "        indices: liste des indices des caractères à charger (0-9: chiffres, 10-35: lettres A-Z)\n",
    "    Retourne:\n",
    "        X: matrice des données (n_samples, n_pixels)\n",
    "   \n",
    "    \"\"\"\n",
    "    try:\n",
    "        mat = scipy.io.loadmat('binaryalphadigs.mat')\n",
    "        data = mat['dat']\n",
    "        \n",
    "        X = []\n",
    "        num_classes, num_samples = data.shape\n",
    "        \n",
    "        if indices is None:\n",
    "            indices = range(num_classes)\n",
    "            \n",
    "        for i in indices:\n",
    "            for j in range(num_samples):\n",
    "                X.append(data[i, j].flatten())\n",
    "                \n",
    "        return np.array(X)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Erreur : Le fichier 'binaryalphadigs.mat' est introuvable.\")\n",
    "        return None\n",
    "\n",
    "def load_mnist_binarized():\n",
    "    \"\"\"\n",
    "    Charge et binarise la base MNIST.\n",
    "    Retourne X_train, y_train, X_test, y_test.\n",
    "   \n",
    "    \"\"\"\n",
    "    print(\"Chargement de MNIST...\")\n",
    "    # Utilisation de scikit-learn pour récupérer MNIST (alternative au lien direct)\n",
    "    mnist = fetch_openml('mnist_784', version=1, as_frame=False, parser='auto')\n",
    "    \n",
    "    X = mnist.data\n",
    "    y = mnist.target.astype(int)\n",
    "    \n",
    "    # Normalisation et Binarisation (seuil à 0.5 après normalisation)\n",
    "    X = X / 255.0\n",
    "    X = (X > 0.5).astype(float)\n",
    "    \n",
    "    # One-hot encoding des labels\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "    y_onehot = encoder.fit_transform(y.reshape(-1, 1))\n",
    "    \n",
    "    # Séparation train/test (60000 train, 10000 test standard)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=10000, random_state=42)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# Fonctions d'activation\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    # Astuce numérique pour la stabilité (soustraire le max)\n",
    "    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return e_x / np.sum(e_x, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c68c74",
   "metadata": {},
   "source": [
    "Implémentation du RBM (Restricted Boltzmann Machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e9cbca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBM:\n",
    "    \"\"\"Structure représentant un RBM avec champs W, a, b [cite: 32]\"\"\"\n",
    "    def __init__(self, W, a, b):\n",
    "        self.W = W # Poids\n",
    "        self.a = a # Biais visible\n",
    "        self.b = b # Biais caché\n",
    "\n",
    "def init_RBM(p, q):\n",
    "    \"\"\"\n",
    "    Initialise les poids et biais d'un RBM.\n",
    "    p: dimension d'entrée (visible)\n",
    "    q: dimension cachée\n",
    "    Poids ~ N(0, 0.01), biais à 0.\n",
    "    [cite: 40, 41, 42]\n",
    "    \"\"\"\n",
    "    W = np.random.normal(0, 0.01, (p, q))\n",
    "    a = np.zeros(p)\n",
    "    b = np.zeros(q)\n",
    "    return RBM(W, a, b)\n",
    "\n",
    "def entree_sortie_RBM(rbm, X):\n",
    "    \"\"\"Calcule P(h=1|v) via sigmoïde [cite: 43]\"\"\"\n",
    "    return sigmoid(X @ rbm.W + rbm.b)\n",
    "\n",
    "def sortie_entree_RBM(rbm, H):\n",
    "    \"\"\"Calcule P(v=1|h) via sigmoïde [cite: 44]\"\"\"\n",
    "    return sigmoid(H @ rbm.W.T + rbm.a)\n",
    "\n",
    "def train_RBM(rbm, X, epochs, lr, batch_size):\n",
    "    \"\"\"\n",
    "    Apprentissage non supervisé par Contrastive-Divergence-1.\n",
    "    [cite: 45, 46]\n",
    "    \"\"\"\n",
    "    n_samples, p = X.shape\n",
    "    q = rbm.W.shape[1]\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        np.random.shuffle(X)\n",
    "        total_error = 0\n",
    "        \n",
    "        for i in range(0, n_samples, batch_size):\n",
    "            batch = X[i:min(i + batch_size, n_samples)]\n",
    "            actual_batch_size = batch.shape[0]\n",
    "            \n",
    "            # 1. Gibbs Sampling (CD-1)\n",
    "            # Positive phase\n",
    "            v0 = batch\n",
    "            p_h0 = entree_sortie_RBM(rbm, v0)\n",
    "            h0 = (np.random.rand(actual_batch_size, q) < p_h0).astype(float)\n",
    "            \n",
    "            # Negative phase\n",
    "            p_v1 = sortie_entree_RBM(rbm, h0)\n",
    "            v1 = (np.random.rand(actual_batch_size, p) < p_v1).astype(float)\n",
    "            p_h1 = entree_sortie_RBM(rbm, v1)\n",
    "            \n",
    "            # 2. Mise à jour des gradients\n",
    "            grad_W = (v0.T @ p_h0 - v1.T @ p_h1) / actual_batch_size\n",
    "            grad_a = np.mean(v0 - v1, axis=0)\n",
    "            grad_b = np.mean(p_h0 - p_h1, axis=0)\n",
    "            \n",
    "            rbm.W += lr * grad_W\n",
    "            rbm.a += lr * grad_a\n",
    "            rbm.b += lr * grad_b\n",
    "            \n",
    "            # Calcul erreur quadratique de reconstruction [cite: 46]\n",
    "            total_error += np.mean((v0 - v1)**2)\n",
    "            \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"RBM Epoch {epoch}/{epochs} - Reconstruction Error: {total_error / (n_samples / batch_size):.4f}\")\n",
    "            \n",
    "    return rbm\n",
    "\n",
    "def generer_image_RBM(rbm, n_iter_gibbs, n_images):\n",
    "    \"\"\"Génère des images via l'échantillonneur de Gibbs [cite: 47, 48]\"\"\"\n",
    "    p = rbm.W.shape[0]\n",
    "    q = rbm.W.shape[1]\n",
    "    \n",
    "    # Initialisation aléatoire\n",
    "    v = (np.random.rand(n_images, p) < 0.5).astype(float)\n",
    "    \n",
    "    for _ in range(n_iter_gibbs):\n",
    "        h_prob = entree_sortie_RBM(rbm, v)\n",
    "        h = (np.random.rand(n_images, q) < h_prob).astype(float)\n",
    "        \n",
    "        v_prob = sortie_entree_RBM(rbm, h)\n",
    "        v = (np.random.rand(n_images, p) < v_prob).astype(float)\n",
    "        \n",
    "    # Affichage\n",
    "    plt.figure(figsize=(10, 2))\n",
    "    for i in range(n_images):\n",
    "        plt.subplot(1, n_images, i+1)\n",
    "        # On suppose des images carrées pour l'affichage (ex: 20x16 pour AlphaDigits ou 28x28 pour MNIST)\n",
    "        dim = int(np.sqrt(p))\n",
    "        if p == 784: dim = 28 # MNIST\n",
    "        elif p == 320: dim_y, dim_x = 20, 16 # AlphaDigit\n",
    "        else: dim_y, dim_x = dim, dim\n",
    "        \n",
    "        if p == 320:\n",
    "             plt.imshow(v[i].reshape(dim_y, dim_x), cmap='gray')\n",
    "        else:\n",
    "             plt.imshow(v[i].reshape(dim, dim), cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.title(f\"Génération RBM (iter={n_iter_gibbs})\")\n",
    "    plt.show()\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b03163b",
   "metadata": {},
   "source": [
    "Implémentation du DBN (Deep Belief Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85027848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur : Le fichier 'binaryalphadigs.mat' est introuvable.\n",
      "Impossible de lancer l'étude préliminaire : fichier .mat manquant.\n"
     ]
    }
   ],
   "source": [
    "# === CELLULE 3-BIS : Étude Préliminaire (Binary AlphaDigits) ===\n",
    "# Cette étape valide que vos RBM/DBN génèrent bien des caractères (Section 4 du PDF)\n",
    "\n",
    "# 1. Charger des données spécifiques (ex: index 10 = 'A', index 11 = 'B')\n",
    "# On concatène les données de A et B pour voir si le réseau apprend les deux\n",
    "data_alpha = lire_alpha_digit([10, 11]) # Indices pour A et B\n",
    "\n",
    "if data_alpha is not None:\n",
    "    n_data, input_dim = data_alpha.shape\n",
    "    print(f\"Données chargées: {n_data} images de dimension {input_dim}\")\n",
    "\n",
    "    # --- Test RBM sur AlphaDigits ---\n",
    "    print(\"\\n--- Apprentissage RBM (AlphaDigits) ---\")\n",
    "    # On configure un RBM avec p entrées et q neurones cachés (ex: 100)\n",
    "    rbm_alpha = init_RBM(p=input_dim, q=100)\n",
    "    \n",
    "    # Entrainement (augmenter epochs si l'image générée est bruitée)\n",
    "    rbm_alpha = train_RBM(rbm_alpha, data_alpha, epochs=100, lr=0.1, batch_size=10)\n",
    "    \n",
    "    # Génération\n",
    "    print(\"Génération via RBM :\")\n",
    "    generer_image_RBM(rbm_alpha, n_iter_gibbs=100, n_images=5)\n",
    "\n",
    "    # --- Test DBN sur AlphaDigits ---\n",
    "    print(\"\\n--- Apprentissage DBN (AlphaDigits) ---\")\n",
    "    # On configure un DBN [320, 200, 100]\n",
    "    dbn_alpha = init_DBN([input_dim, 200, 100])\n",
    "    \n",
    "    # Entrainement\n",
    "    dbn_alpha = train_DBN(dbn_alpha, data_alpha, epochs=100, lr=0.1, batch_size=10)\n",
    "    \n",
    "    # Génération\n",
    "    print(\"Génération via DBN :\")\n",
    "    generer_image_DBN(dbn_alpha, n_iter_gibbs=100, n_images=5)\n",
    "else:\n",
    "    print(\"Impossible de lancer l'étude préliminaire : fichier .mat manquant.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3134af43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBN:\n",
    "    \"\"\"Liste de RBMs [cite: 33]\"\"\"\n",
    "    def __init__(self, rbms):\n",
    "        self.rbms = rbms\n",
    "\n",
    "def init_DBN(layer_sizes):\n",
    "    \"\"\"\n",
    "    Construit et initialise un DBN.\n",
    "    layer_sizes: liste des tailles [input, hidden1, hidden2, ...]\n",
    "    Utilise itérativement init_RBM. [cite: 54, 55]\n",
    "    \"\"\"\n",
    "    rbms = []\n",
    "    for i in range(len(layer_sizes) - 1):\n",
    "        rbm = init_RBM(layer_sizes[i], layer_sizes[i+1])\n",
    "        rbms.append(rbm)\n",
    "    return DBN(rbms)\n",
    "\n",
    "def train_DBN(dbn, X, epochs, lr, batch_size):\n",
    "    \"\"\"\n",
    "    Apprentissage Greedy Layer-Wise. [cite: 56, 57]\n",
    "    Entraine chaque RBM successivement. [cite: 58]\n",
    "    \"\"\"\n",
    "    input_data = X\n",
    "    \n",
    "    for i, rbm in enumerate(dbn.rbms):\n",
    "        print(f\"--- Entrainement Couche DBN {i+1} ---\")\n",
    "        # Entrainement du RBM courant\n",
    "        dbn.rbms[i] = train_RBM(rbm, input_data, epochs, lr, batch_size)\n",
    "        \n",
    "        # Calcul de la sortie pour la couche suivante (moyenne activée)\n",
    "        input_data = entree_sortie_RBM(dbn.rbms[i], input_data)\n",
    "        \n",
    "    return dbn\n",
    "\n",
    "def generer_image_DBN(dbn, n_iter_gibbs, n_images):\n",
    "    \"\"\"\n",
    "    Génère des échantillons suivant un DBN. [cite: 60, 61]\n",
    "    Stratégie: Générer sur le dernier RBM, puis redescendre.\n",
    "    \"\"\"\n",
    "    # 1. Échantillonnage de Gibbs sur le dernier RBM (niveau le plus profond)\n",
    "    last_rbm = dbn.rbms[-1]\n",
    "    p_last = last_rbm.W.shape[0] # Entrée du dernier RBM\n",
    "    q_last = last_rbm.W.shape[1] # Caché du dernier RBM\n",
    "    \n",
    "    # Init aléatoire au niveau top\n",
    "    h = (np.random.rand(n_images, q_last) < 0.5).astype(float)\n",
    "    \n",
    "    # Gibbs sur le dernier niveau\n",
    "    for _ in range(n_iter_gibbs):\n",
    "        v_prob = sortie_entree_RBM(last_rbm, h)\n",
    "        v = (np.random.rand(n_images, p_last) < v_prob).astype(float)\n",
    "        h_prob = entree_sortie_RBM(last_rbm, v)\n",
    "        h = (np.random.rand(n_images, q_last) < h_prob).astype(float)\n",
    "        \n",
    "    # Le 'v' obtenu est l'entrée de la dernière couche. Il faut redescendre le réseau.\n",
    "    current_input = v\n",
    "    \n",
    "    # 2. Propagation descendante (Top-Down)\n",
    "    for i in range(len(dbn.rbms) - 1, -1, -1):\n",
    "        rbm = dbn.rbms[i]\n",
    "        # On utilise sortie_entree_RBM pour passer de la couche cachée vers visible\n",
    "        probs = sortie_entree_RBM(rbm, current_input)\n",
    "        # Binarisation (sauf peut-être pour la couche visible finale si on veut du niveau de gris)\n",
    "        current_input = (np.random.rand(n_images, rbm.W.shape[0]) < probs).astype(float)\n",
    "        \n",
    "    # Affichage\n",
    "    v_final = current_input\n",
    "    p = v_final.shape[1]\n",
    "    plt.figure(figsize=(10, 2))\n",
    "    for i in range(n_images):\n",
    "        plt.subplot(1, n_images, i+1)\n",
    "        dim = int(np.sqrt(p))\n",
    "        if p == 784: dim = 28\n",
    "        elif p == 320: dim_y, dim_x = 20, 16\n",
    "        else: dim_y, dim_x = dim, dim\n",
    "        \n",
    "        if p == 320:\n",
    "             plt.imshow(v_final[i].reshape(dim_y, dim_x), cmap='gray')\n",
    "        else:\n",
    "             plt.imshow(v_final[i].reshape(dim, dim), cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.title(f\"Génération DBN (iter={n_iter_gibbs})\")\n",
    "    plt.show()\n",
    "    return v_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1737ecb",
   "metadata": {},
   "source": [
    "Implémentation du DNN (Deep Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58e62682",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN:\n",
    "    \"\"\"DNN = DBN + Couche de classification [cite: 66]\"\"\"\n",
    "    def __init__(self, dbn, classification_layer=None):\n",
    "        self.dbn = dbn\n",
    "        # Classification layer: Poids et biais pour Softmax\n",
    "        self.W_class = classification_layer['W'] if classification_layer else None\n",
    "        self.b_class = classification_layer['b'] if classification_layer else None\n",
    "\n",
    "def init_DNN(layer_sizes):\n",
    "    \"\"\"\n",
    "    layer_sizes: [input, hidden1, ..., hiddenN, output_classes]\n",
    "    Retourne un DNN initialisé. [cite: 65, 66]\n",
    "    \"\"\"\n",
    "    # La partie DBN concerne tout sauf la dernière couche (classification)\n",
    "    dbn_sizes = layer_sizes[:-1]\n",
    "    dbn = init_DBN(dbn_sizes)\n",
    "    \n",
    "    # Initialisation couche classification\n",
    "    input_dim = layer_sizes[-2]\n",
    "    output_dim = layer_sizes[-1]\n",
    "    W_class = np.random.normal(0, 0.01, (input_dim, output_dim))\n",
    "    b_class = np.zeros(output_dim)\n",
    "    \n",
    "    return DNN(dbn, {'W': W_class, 'b': b_class})\n",
    "\n",
    "def pretrain_DNN(dnn, X, epochs, lr, batch_size):\n",
    "    \"\"\"Pré-entrainement des couches cachées via train_DBN [cite: 67, 68]\"\"\"\n",
    "    dnn.dbn = train_DBN(dnn.dbn, X, epochs, lr, batch_size)\n",
    "    return dnn\n",
    "\n",
    "def calcul_softmax(X, dnn):\n",
    "    \"\"\"Calcule probabilités sortie via softmax [cite: 69]\"\"\"\n",
    "    Z = X @ dnn.W_class + dnn.b_class\n",
    "    return softmax(Z)\n",
    "\n",
    "def entree_sortie_reseau(dnn, X):\n",
    "    \"\"\"\n",
    "    Passe avant (Forward pass).\n",
    "    Retourne une liste contenant les sorties de chaque couche.\n",
    "    [cite: 71, 72]\n",
    "    \"\"\"\n",
    "    activations = [X]\n",
    "    current_input = X\n",
    "    \n",
    "    # Passage dans le DBN (sigmoïdes)\n",
    "    for rbm in dnn.dbn.rbms:\n",
    "        current_input = entree_sortie_RBM(rbm, current_input)\n",
    "        activations.append(current_input)\n",
    "        \n",
    "    # Passage dans la couche de classification (Softmax)\n",
    "    probs = calcul_softmax(current_input, dnn)\n",
    "    activations.append(probs)\n",
    "    \n",
    "    return activations\n",
    "\n",
    "def retropropagation(dnn, X, y, epochs, lr, batch_size):\n",
    "    \"\"\"\n",
    "    Apprentissage supervisé (Fine-tuning). [cite: 73, 74]\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Mélange des données\n",
    "        indices = np.arange(n_samples)\n",
    "        np.random.shuffle(indices)\n",
    "        X_shuffled = X[indices]\n",
    "        y_shuffled = y[indices]\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for i in range(0, n_samples, batch_size):\n",
    "            X_batch = X_shuffled[i:i+batch_size]\n",
    "            y_batch = y_shuffled[i:i+batch_size]\n",
    "            batch_len = X_batch.shape[0]\n",
    "            \n",
    "            # 1. Forward Pass\n",
    "            activations = entree_sortie_reseau(dnn, X_batch)\n",
    "            y_pred = activations[-1] # Sortie Softmax\n",
    "            \n",
    "            # 2. Calcul de l'erreur (Entropie croisée) [cite: 74]\n",
    "            # Loss = -sum(y_true * log(y_pred))\n",
    "            loss = -np.sum(y_batch * np.log(y_pred + 1e-9)) / batch_len\n",
    "            epoch_loss += loss\n",
    "            \n",
    "            # 3. Backward Pass\n",
    "            \n",
    "            # Delta couche sortie (Softmax + CrossEntropy derivative = pred - true)\n",
    "            delta = (y_pred - y_batch) / batch_len\n",
    "            \n",
    "            # Gradient pour la couche de classification\n",
    "            grad_W_class = activations[-2].T @ delta\n",
    "            grad_b_class = np.sum(delta, axis=0)\n",
    "            \n",
    "            # Rétropropagation à travers le DBN\n",
    "            # W_new = W_old - lr * grad\n",
    "            # Note: On doit stocker les gradients avant de mettre à jour pour l'étape précédente\n",
    "            \n",
    "            prev_W_class = dnn.W_class.copy() # Sauvegarde pour calcul delta couche précédente\n",
    "            \n",
    "            # Mise à jour couche classification\n",
    "            dnn.W_class -= lr * grad_W_class\n",
    "            dnn.b_class -= lr * grad_b_class\n",
    "            \n",
    "            # Propagation vers les couches cachées\n",
    "            # delta_hidden = (delta @ W_next.T) * sigmoid'(a)\n",
    "            # sigmoid'(x) = x * (1 - x) pour x étant la sortie de la sigmoïde\n",
    "            \n",
    "            delta_curr = delta\n",
    "            W_next = prev_W_class\n",
    "            \n",
    "            # On itère à l'envers sur les RBMs\n",
    "            for j in range(len(dnn.dbn.rbms) - 1, -1, -1):\n",
    "                rbm = dnn.dbn.rbms[j]\n",
    "                output_j = activations[j+1] # Sortie de la couche actuelle\n",
    "                input_j = activations[j]    # Entrée de la couche actuelle\n",
    "                \n",
    "                # Dérivée sigmoïde\n",
    "                sig_deriv = output_j * (1 - output_j)\n",
    "                \n",
    "                # Calcul delta\n",
    "                delta_prev = (delta_curr @ W_next.T) * sig_deriv\n",
    "                \n",
    "                # Gradients\n",
    "                grad_W = input_j.T @ delta_prev\n",
    "                grad_b = np.sum(delta_prev, axis=0) # biais caché 'b' du RBM\n",
    "                # Note: Le biais 'a' du RBM n'est pas utilisé en feedforward standard DNN (sauf si on considère l'entrée)\n",
    "                \n",
    "                # Mise à jour\n",
    "                W_next = rbm.W.copy() # Pour la couche d'avant\n",
    "                rbm.W -= lr * grad_W\n",
    "                rbm.b -= lr * grad_b\n",
    "                \n",
    "                delta_curr = delta_prev\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Retroprop Epoch {epoch}/{epochs} - Cross Entropy Loss: {epoch_loss:.4f}\")\n",
    "            \n",
    "    return dnn\n",
    "\n",
    "def test_DNN(dnn, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Test du réseau et calcul du taux d'erreur.\n",
    "    [cite: 75, 77, 78]\n",
    "    \"\"\"\n",
    "    activations = entree_sortie_reseau(dnn, X_test)\n",
    "    y_pred_proba = activations[-1]\n",
    "    \n",
    "    # Conversion One-hot -> Classe\n",
    "    pred_labels = np.argmax(y_pred_proba, axis=1)\n",
    "    true_labels = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    error_rate = np.mean(pred_labels != true_labels)\n",
    "    return error_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a5d311",
   "metadata": {},
   "source": [
    "Script Principal pour Analyse Compararative (MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c816ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement de MNIST...\n",
      "\n",
      "=== Figure 1: Analysis by Depth ===\n",
      "\n",
      "--- Config: [784, 200, 200, 10], Data: 5000 ---\n",
      "Training DNN WITH Pre-training...\n",
      "--- Entrainement Couche DBN 1 ---\n",
      "RBM Epoch 0/10 - Reconstruction Error: 0.2006\n"
     ]
    }
   ],
   "source": [
    "# Paramètres globaux [cite: 95]\n",
    "INPUT_DIM = 784 # MNIST\n",
    "OUTPUT_DIM = 10\n",
    "LR_RBM = 0.1\n",
    "LR_DNN = 0.1\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS_RBM = 10 # Réduit pour tester rapidement (mettre 100+ pour rendu final)\n",
    "EPOCHS_DNN = 20 # Réduit pour tester rapidement (mettre 200 pour rendu final)\n",
    "\n",
    "# Chargement données\n",
    "X_train_full, y_train_full, X_test, y_test = load_mnist_binarized()\n",
    "\n",
    "def run_comparison_experiment(hidden_layers_cfg, train_size):\n",
    "    \"\"\"\n",
    "    Exécute la comparaison Pre-trained vs Random Init.\n",
    "    Retourne (erreur_pretrained, erreur_random)\n",
    "    \"\"\"\n",
    "    # Sous-échantillonnage des données train selon train_size [cite: 113]\n",
    "    indices = np.random.choice(X_train_full.shape[0], train_size, replace=False)\n",
    "    X_train = X_train_full[indices]\n",
    "    y_train = y_train_full[indices]\n",
    "    \n",
    "    config = [INPUT_DIM] + hidden_layers_cfg + [OUTPUT_DIM]\n",
    "    print(f\"\\n--- Config: {config}, Data: {train_size} ---\")\n",
    "    \n",
    "    # 1. Réseau avec Pré-entrainement [cite: 105, 106]\n",
    "    print(\"Training DNN WITH Pre-training...\")\n",
    "    dnn_pre = init_DNN(config)\n",
    "    dnn_pre = pretrain_DNN(dnn_pre, X_train, epochs=EPOCHS_RBM, lr=LR_RBM, batch_size=BATCH_SIZE)\n",
    "    dnn_pre = retropropagation(dnn_pre, X_train, y_train, epochs=EPOCHS_DNN, lr=LR_DNN, batch_size=BATCH_SIZE)\n",
    "    err_pre = test_DNN(dnn_pre, X_test, y_test)\n",
    "    \n",
    "    # 2. Réseau Initialisation Aléatoire [cite: 107]\n",
    "    print(\"Training DNN WITHOUT Pre-training...\")\n",
    "    dnn_rand = init_DNN(config)\n",
    "    dnn_rand = retropropagation(dnn_rand, X_train, y_train, epochs=EPOCHS_DNN, lr=LR_DNN, batch_size=BATCH_SIZE)\n",
    "    err_rand = test_DNN(dnn_rand, X_test, y_test)\n",
    "    \n",
    "    print(f\"Error Pre-trained: {err_pre:.4f} | Error Random: {err_rand:.4f}\")\n",
    "    return err_pre, err_rand\n",
    "\n",
    "# --- Exécution des Analyses (Figures) ---\n",
    "\n",
    "# Figure 1: Variation du nombre de couches [cite: 111]\n",
    "# Fixe: 200 neurones/couche, 10000 data. Variable: nb couches.\n",
    "nb_layers_list = [2, 3, 4] # Exemple: 2 couches cachées, puis 3...\n",
    "res_fig1 = []\n",
    "print(\"\\n=== Figure 1: Analysis by Depth ===\")\n",
    "for l in nb_layers_list:\n",
    "    cfg = [200] * l # l couches de 200\n",
    "    res_fig1.append(run_comparison_experiment(cfg, 5000)) # 5000 données pour rapidité\n",
    "\n",
    "# Figure 2: Variation du nombre de neurones [cite: 112]\n",
    "# Fixe: 2 couches, 10000 data. Variable: nb neurones.\n",
    "neurons_list = [100, 300, 500]\n",
    "res_fig2 = []\n",
    "print(\"\\n=== Figure 2: Analysis by Width ===\")\n",
    "for n in neurons_list:\n",
    "    cfg = [n, n] # 2 couches de n\n",
    "    res_fig2.append(run_comparison_experiment(cfg, 5000))\n",
    "\n",
    "# Figure 3: Variation du nombre de données [cite: 113]\n",
    "# Fixe: 2 couches de 200. Variable: data size.\n",
    "data_sizes = [1000, 3000, 7000]\n",
    "res_fig3 = []\n",
    "print(\"\\n=== Figure 3: Analysis by Data Size ===\")\n",
    "for sz in data_sizes:\n",
    "    cfg = [200, 200]\n",
    "    res_fig3.append(run_comparison_experiment(cfg, sz))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296cdb93",
   "metadata": {},
   "source": [
    "Affichage des Résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1d8af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction utilitaire de plot\n",
    "def plot_results(x_values, results, x_label, title):\n",
    "    pre = [r[0] for r in results]\n",
    "    rand = [r[1] for r in results]\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(x_values, pre, 'o-', label='Pré-entrainé')\n",
    "    plt.plot(x_values, rand, 's-', label='Init Aléatoire')\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(\"Taux d'erreur (Test)\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Affichage Fig 1\n",
    "plot_results(nb_layers_list, res_fig1, \"Nombre de couches cachées\", \"Erreur vs Profondeur\")\n",
    "\n",
    "# Affichage Fig 2\n",
    "plot_results(neurons_list, res_fig2, \"Neurones par couche\", \"Erreur vs Largeur\")\n",
    "\n",
    "# Affichage Fig 3\n",
    "plot_results(data_sizes, res_fig3, \"Taille du jeu d'entrainement\", \"Erreur vs Données\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
